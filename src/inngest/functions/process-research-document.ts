/**
 * Process research documents: chunk text and generate embeddings.
 *
 * This function processes JSON research reports generated by the research agent.
 * It reads the JSON file from storage, extracts relevant text, chunks it, and generates embeddings.
 *
 * Flow:
 * 1. Download JSON file from sustainability-reports bucket
 * 2. Extract text content from JSON structure
 * 3. Chunk text into segments (500 tokens with 50 token overlap)
 * 4. Generate embeddings in batches (100 chunks at a time)
 * 5. Insert segments into database with embeddings
 * 6. Link segments to research_segments table
 * 7. Update research_documents status to completed
 */

import { inngest } from '../client';
import { createClient } from '@/lib/supabase/server';
import { chunkTextSmart } from '@/lib/chunking';
import { generateEmbeddings, formatEmbeddingForPostgres } from '@/lib/embeddings';

interface ResearchDocumentEvent {
  researchDocumentId: string;
}

export const processResearchDocument = inngest.createFunction(
  {
    id: 'process-research-document',
    retries: 3,
    concurrency: {
      limit: 5, // Limit concurrent embedding generation (OpenAI rate limits)
    },
    onFailure: async ({ event, error }) => {
      // Mark document as failed in database
      const supabase = await createClient();
      const { researchDocumentId } = event.data as ResearchDocumentEvent;
      await supabase
        .from('research_documents')
        .update({
          vectorization_status: 'failed',
        })
        .eq('id', researchDocumentId);
    },
  },
  { event: 'research/document.created' },
  async ({ event, step }) => {
    const { researchDocumentId } = event.data as ResearchDocumentEvent;

    // Step 1: Get research document details
    const researchDocument = await step.run('get-research-document', async () => {
      const supabase = await createClient();

      const { data: doc, error } = await supabase
        .from('research_documents')
        .select('*')
        .eq('id', researchDocumentId)
        .single();

      if (error || !doc) {
        throw new Error(`Research document not found: ${researchDocumentId}`);
      }

      // Update status to processing
      await supabase
        .from('research_documents')
        .update({
          vectorization_status: 'processing',
        })
        .eq('id', researchDocumentId);

      return doc;
    });

    // Step 2: Download JSON file from storage
    const fileContent = await step.run('download-file', async () => {
      const supabase = await createClient();

      const { data, error } = await supabase.storage
        .from(researchDocument.storage_bucket)
        .download(researchDocument.file_path);

      if (error || !data) {
        throw new Error(`Failed to download file: ${error?.message}`);
      }

      // Convert Blob to text
      return await data.text();
    });

    // Step 3: Extract text from JSON
    const extractedText = await step.run('extract-text', async () => {
      try {
        const jsonData = JSON.parse(fileContent);

        // Extract all text content from the JSON
        // This will concatenate all string values in the JSON
        const extractTextFromObject = (obj: any): string => {
          if (typeof obj === 'string') {
            return obj + ' ';
          }
          if (Array.isArray(obj)) {
            return obj.map(extractTextFromObject).join(' ');
          }
          if (obj && typeof obj === 'object') {
            return Object.values(obj).map(extractTextFromObject).join(' ');
          }
          return '';
        };

        const text = extractTextFromObject(jsonData);

        // Add metadata prefix for context
        const prefix = `Company: ${researchDocument.company_name}\nCategory: ${researchDocument.category}\n\n`;
        return prefix + text;
      } catch (error) {
        throw new Error(`Failed to parse JSON: ${error}`);
      }
    });

    // Step 4: Chunk the text
    const chunks = await step.run('chunk-text', async () => {
      const textChunks = chunkTextSmart(extractedText, {
        chunkSize: 500,
        overlap: 50,
      });

      return textChunks;
    });

    // Step 5: Generate embeddings
    const embeddings = await step.run('generate-embeddings', async () => {
      return await generateEmbeddings(chunks);
    });

    // Step 6: Get user_id from research_queue
    const userId = await step.run('get-user-id', async () => {
      const supabase = await createClient();

      const { data: queueEntry, error } = await supabase
        .from('research_queue')
        .select('user_id')
        .eq('id', researchDocument.research_id)
        .single();

      if (error || !queueEntry) {
        throw new Error('Could not find research queue entry');
      }

      return queueEntry.user_id;
    });

    // Step 7: Insert segments into database
    const segmentIds = await step.run('insert-segments', async () => {
      const supabase = await createClient();

      // Create a dummy document and class for research segments
      // These are required by the segments table foreign key constraints
      // but aren't actually used for research documents
      const dummyClassId = '00000000-0000-0000-0000-000000000001';
      const dummyDocumentId = '00000000-0000-0000-0000-000000000001';

      // Prepare segment rows
      const segments = chunks.map((chunk, index) => ({
        user_id: userId,
        class_id: dummyClassId,
        document_id: dummyDocumentId,
        content: chunk.content,
        embedding: formatEmbeddingForPostgres(embeddings[index]),
        segment_index: chunk.segmentIndex,
        char_start: chunk.charStart,
        char_end: chunk.charEnd,
        embedding_model: 'text-embedding-3-small',
      }));

      // Insert segments in batches
      const batchSize = 100;
      const insertedIds: string[] = [];

      for (let i = 0; i < segments.length; i += batchSize) {
        const batch = segments.slice(i, i + batchSize);
        const { data, error } = await supabase
          .from('segments')
          .insert(batch)
          .select('id');

        if (error) {
          throw new Error(`Failed to insert segments: ${error.message}`);
        }

        insertedIds.push(...(data || []).map((s) => s.id));
      }

      return insertedIds;
    });

    // Step 8: Link segments to research_segments table
    await step.run('link-segments', async () => {
      const supabase = await createClient();

      const researchSegments = segmentIds.map((segmentId) => ({
        research_document_id: researchDocumentId,
        segment_id: segmentId,
        company_name: researchDocument.company_name,
        category: researchDocument.category,
      }));

      const { error } = await supabase
        .from('research_segments')
        .insert(researchSegments);

      if (error) {
        throw new Error(`Failed to link segments: ${error.message}`);
      }
    });

    // Step 9: Update research_documents status
    await step.run('mark-completed', async () => {
      const supabase = await createClient();

      await supabase
        .from('research_documents')
        .update({
          vectorization_status: 'completed',
          segment_count: chunks.length,
        })
        .eq('id', researchDocumentId);
    });

    return {
      success: true,
      researchDocumentId,
      segmentsCreated: chunks.length,
    };
  }
);
